{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa44db3-26e8-4536-82c1-d4a80457a469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV from GCS...\n",
      "Loaded 3000 rows\n",
      "Cleaning text...\n",
      "Encoding labels...\n",
      "Splitting dataset...\n",
      "Loading tokenizer...\n",
      "Creating datasets...\n",
      "Initializing model...\n",
      "Training model...\n",
      "\n",
      "Epoch 1 Training...\n",
      "Epoch 1 complete.\n",
      "\n",
      "Evaluating on validation set...\n",
      "\n",
      "Validation Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Compliance       1.00      0.99      1.00       200\n",
      "          IP       0.96      0.99      0.97       200\n",
      "     Privacy       0.98      0.95      0.97       200\n",
      "\n",
      "    accuracy                           0.98       600\n",
      "   macro avg       0.98      0.98      0.98       600\n",
      "weighted avg       0.98      0.98      0.98       600\n",
      "\n",
      "\n",
      "Saving model artifacts locally...\n",
      "\n",
      "Uploading artifacts to GCS bucket root...\n",
      "Uploaded: model_artifacts/model.pth --> gs://contract-risk-classifier/model.pth\n",
      "Uploaded: model_artifacts/tokenizer.pkl --> gs://contract-risk-classifier/tokenizer.pkl\n",
      "Uploaded: model_artifacts/label_encoder.pkl --> gs://contract-risk-classifier/label_encoder.pkl\n",
      "\n",
      "ðŸŽ‰ All done. Artifacts uploaded to GCS bucket root.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from google.cloud import storage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------\n",
    "# Configs\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "ARTIFACTS_DIR = \"model_artifacts\"\n",
    "BUCKET_NAME = 'contract-risk-classifier'\n",
    "\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def data_cleaning(sentence, legal_terms=None, custom_stop_words=None):\n",
    "    if legal_terms is None:\n",
    "        legal_terms = {'agreement': 'contract', 'parties': 'party'}\n",
    "    if custom_stop_words is None:\n",
    "        custom_stop_words = {'shall', 'hereinafter'}\n",
    "    stop_words = nlp.Defaults.stop_words.union(custom_stop_words)\n",
    "\n",
    "    sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('ascii')\n",
    "    sentence = contractions.fix(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', sentence)\n",
    "    sentence = re.sub(r'<[^>]+>', '', sentence)\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in stop_words and len(token.lemma_) > 1]\n",
    "    tokens = [legal_terms.get(token, token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class ContractDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------------------------\n",
    "# Model\n",
    "# -------------------------------\n",
    "class ContractRiskClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ContractRiskClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # Freeze all\n",
    "        for param in self.bert.encoder.layer[-1].parameters():\n",
    "            param.requires_grad = True  # Unfreeze last layer only\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "# -------------------------------\n",
    "# Train & Eval\n",
    "# -------------------------------\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1} Training...\")\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids, mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} complete.\")\n",
    "\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, mask)\n",
    "            predictions = output.argmax(dim=1)\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nValidation Report:\\n\", classification_report(targets, preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# -------------------------------\n",
    "# Upload to GCS\n",
    "# -------------------------------\n",
    "def upload_to_gcs(local_path, blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded: {local_path} --> gs://{BUCKET_NAME}/{blob_name}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading CSV from GCS...\")\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(\"cleaned_contract_data.csv\")\n",
    "    data_bytes = blob.download_as_bytes()\n",
    "    data = pd.read_csv(io.BytesIO(data_bytes))\n",
    "    privacy_data = data[data['risk'] == 'Privacy'].head(1000)\n",
    "    ip_data = data[data['risk'] == 'IP'].head(1000)\n",
    "    compliance_data = data[data['risk'] == 'Compliance'].head(1000)\n",
    "    df = pd.concat([privacy_data, ip_data, compliance_data])\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "    print(\"Cleaning text...\")\n",
    "    df['cleaned_text'] = df['text'].apply(data_cleaning)\n",
    "\n",
    "    print(\"Encoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['risk'])\n",
    "\n",
    "    print(\"Splitting dataset...\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(\"Creating datasets...\")\n",
    "    train_data = ContractDataset(train_df['cleaned_text'].tolist(), train_df['label'].tolist(), tokenizer)\n",
    "    val_data = ContractDataset(val_df['cleaned_text'].tolist(), val_df['label'].tolist(), tokenizer)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = ContractRiskClassifier(n_classes=len(label_encoder.classes_))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    train_model(model, train_loader, val_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Save Artifacts Locally\n",
    "    # -------------------------------\n",
    "    print(\"\\nSaving model artifacts locally...\")\n",
    "    torch.save(model.state_dict(), f\"{ARTIFACTS_DIR}/model.pth\")\n",
    "    joblib.dump(tokenizer, f\"{ARTIFACTS_DIR}/tokenizer.pkl\")\n",
    "    joblib.dump(label_encoder, f\"{ARTIFACTS_DIR}/label_encoder.pkl\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Upload to GCS (bucket root)\n",
    "    # -------------------------------\n",
    "    print(\"\\nUploading artifacts to GCS bucket root...\")\n",
    "    upload_to_gcs(f\"{ARTIFACTS_DIR}/model.pth\", \"model.pth\")\n",
    "    upload_to_gcs(f\"{ARTIFACTS_DIR}/tokenizer.pkl\", \"tokenizer.pkl\")\n",
    "    upload_to_gcs(f\"{ARTIFACTS_DIR}/label_encoder.pkl\", \"label_encoder.pkl\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ All done. Artifacts uploaded to GCS bucket root.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee5ec71a-4725-4048-bfcf-591d7228dabd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Input: The contractor shall be liable  for any breach of  disclosed during the term of this agreement.\n",
      "ðŸ“Š Predicted Risk Level: Privacy\n"
     ]
    }
   ],
   "source": [
    "# Single test sentence\n",
    "test_sentence = \"The contractor shall be liable  for any breach of confidentiality disclosed during the term of this agreement.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned = data_cleaning(test_sentence)\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer.encode_plus(\n",
    "    cleaned,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Run inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    pred_label_id = outputs.argmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "# Decode label\n",
    "predicted_label = label_encoder.inverse_transform([pred_label_id])[0]\n",
    "\n",
    "print(f\"ðŸ”Ž Input: {test_sentence}\")\n",
    "print(f\"ðŸ“Š Predicted Risk Level: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75ea61bb-14a5-4d2d-8910-baab9dc284f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Privacy', 'IP', 'Compliance'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['risk'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd9478-737a-45a7-a7c0-30447e8fc762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
